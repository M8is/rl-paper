@inproceedings{Peters2010PolicyGM,
  title={Policy Gradient Methods},
  author={Jan Peters and J. Andrew Bagnell},
  booktitle={Encyclopedia of Machine Learning},
  year={2010}
}

@inproceedings{Kimura1998AnAO,
  title={An Analysis of Actor/Critic Algorithms Using Eligibility Traces: Reinforcement Learning with Imperfect Value Function},
  author={Hajime Kimura and Shigenobu Kobayashi},
  booktitle={ICML},
  year={1998}
}

@incollection{NIPS2015_5672,
title = {Model-Based Relative Entropy Stochastic Search},
author = {Abdolmaleki, Abbas and Lioutikov, Rudolf and Peters, Jan R and Lau, Nuno and Pualo Reis, Luis and Neumann, Gerhard},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {3537--3545},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5672-model-based-relative-entropy-stochastic-search.pdf}
}

@article{DBLP:journals/corr/abs-1808-03030,
  author    = {Ruiyi Zhang and
               Changyou Chen and
               Chunyuan Li and
               Lawrence Carin},
  title     = {Policy Optimization as Wasserstein Gradient Flows},
  journal   = {CoRR},
  volume    = {abs/1808.03030},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03030},
  archivePrefix = {arXiv},
  eprint    = {1808.03030},
  timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-03030},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanMLJA15,
  author    = {John Schulman and
               Philipp Moritz and
               Sergey Levine and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  journal   = {CoRR},
  volume    = {abs/1506.02438},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02438},
  archivePrefix = {arXiv},
  eprint    = {1506.02438},
  timestamp = {Mon, 13 Aug 2018 16:46:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanMLJA15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{abdolmaleki2018maximum,
title={Maximum a Posteriori Policy Optimisation},
author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1ANxQW0b},
}

@incollection{levenbergmarquardt1978,
  year={1978},
  added-at = {2015-02-27T13:06:26.000+0100},
  author = {Mor√©, JorgeJ.},
  biburl = {https://www.bibsonomy.org/bibtex/2aaee107544f8796586ef03d8d6ff3775/olavg},
  booktitle = {Numerical Analysis},
  doi = {10.1007/BFb0067700},
  editor = {Watson, G.A.},
  interhash = {db8074612ab7de61d914de0200a548b4},
  intrahash = {aaee107544f8796586ef03d8d6ff3775},
  isbn = {978-3-540-08538-6},
  keywords = {algorithm implementation levenberg marquardt theory},
  language = {English},
  pages = {105-116},
  publisher = {Springer Berlin Heidelberg},
  series = {Lecture Notes in Mathematics},
  timestamp = {2015-02-27T13:06:26.000+0100},
  title = {The Levenberg-Marquardt algorithm: Implementation and theory},
  url = {http://dx.doi.org/10.1007/BFb0067700},
  volume = 630,
  year = 1978
}

@article{4863,
  title = {Natural Actor-Critic},
  author = {Peters, J. and Schaal, S.},
  journal = {Neurocomputing},
  volume = {71},
  number = {7-9},
  pages = {1180-1190},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  month = mar,
  year = {2008},
  month_numeric = {3}
}

@incollection{NIPS1999_1786,
title = {Actor-Critic Algorithms},
author = {Vijay R. Konda and John N. Tsitsiklis},
booktitle = {Advances in Neural Information Processing Systems 12},
editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
pages = {1008--1014},
year = {2000},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf}
}

@article{Konda:2003:AA:942271.942292,
 author = {Konda, Vijay R. and Tsitsiklis, John N.},
 title = {On Actor-Critic Algorithms},
 journal = {SIAM J. Control Optim.},
 issue_date = {2003},
 volume = {42},
 number = {4},
 month = apr,
 year = {2003},
 issn = {0363-0129},
 pages = {1143--1166},
 numpages = {24},
 url = {https://doi.org/10.1137/S0363012901385691},
 doi = {10.1137/S0363012901385691},
 acmid = {942292},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {Markov decision processes, actor-critic algorithms, reinforcement learning, stochastic approximation},
}

@inproceedings{Peters_IICHR_2003,
  author =	 "Peters, J. and Vijayakumar, S. and Schaal, S.",
  year =	 "2003",
  title = "Reinforcement learning for humanoid robotics",
  booktitle = "IEEE-RAS International Conference on Humanoid Robots (Humanoids2003)",
  key = "reinforcement learning, policy gradients, movement primitives, behaviors, dynamic systems, humanoid robotics",
  URL = "https://www.ias.informatik.tu-darmstadt.de/uploads/Team/JanPeters/peters-ICHR2003.pdf"
}

@article{Lagoudakis:2003:LPI:945365.964290,
 author = {Lagoudakis, Michail G. and Parr, Ronald},
 title = {Least-squares Policy Iteration},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2003},
 volume = {4},
 month = dec,
 year = {2003},
 issn = {1532-4435},
 pages = {1107--1149},
 numpages = {43},
 url = {http://dl.acm.org/citation.cfm?id=945365.964290},
 acmid = {964290},
 publisher = {JMLR.org},
}

@inproceedings{Boyan:1999:LTD:645528.657618,
 author = {Boyan, Justin A.},
 title = {Least-Squares Temporal Difference Learning},
 booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
 series = {ICML '99},
 year = {1999},
 isbn = {1-55860-612-2},
 pages = {49--56},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=645528.657618},
 acmid = {657618},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@InProceedings{10.1007/978-3-540-69162-4_32,
author="Honkela, Antti
and Tornio, Matti
and Raiko, Tapani
and Karhunen, Juha",
editor="Ishikawa, Masumi
and Doya, Kenji
and Miyamoto, Hiroyuki
and Yamakawa, Takeshi",
title="Natural Conjugate Gradient in Variational Inference",
booktitle="Neural Information Processing",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="305--314",
abstract="Variational methods for approximate inference in machine learning often adapt a parametric probability distribution to optimize a given objective function. This view is especially useful when applying variational Bayes (VB) to models outside the conjugate-exponential family. For them, variational Bayesian expectation maximization (VB EM) algorithms are not easily available, and gradient-based methods are often used as alternatives. Traditional natural gradient methods use the Riemannian structure (or geometry) of the predictive distribution to speed up maximum likelihood estimation. We propose using the geometry of the variational approximating distribution instead to speed up a conjugate gradient method for variational learning and inference. The computational overhead is small due to the simplicity of the approximating distribution. Experiments with real-world speech data show significant speedups over alternative learning algorithms.",
isbn="978-3-540-69162-4"
}

@article{DBLP:journals/corr/abs-1301-3584,
  author    = {Razvan Pascanu and
               Yoshua Bengio},
  title     = {Natural Gradient Revisited},
  journal   = {CoRR},
  volume    = {abs/1301.3584},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3584},
  archivePrefix = {arXiv},
  eprint    = {1301.3584},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1301-3584},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{NIPS2017_7233,
title = {Towards Generalization and Simplicity in Continuous Control},
author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel V. and Kakade, Sham M},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6550--6561},     
year = {2017},       
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7233-towards-generalization-and-simplicity-in-continuous-control.pdf}
}

@article{JMLR:v15:wierstra14a,
  author  = {Daan Wierstra and Tom Schaul and Tobias Glasmachers and Yi Sun and Jan Peters and J\"{u}rgen Schmidhuber},
  title   = {Natural Evolution Strategies},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {949-980},
  url     = {http://jmlr.org/papers/v15/wierstra14a.html}
}

@article{4867,
  title = {Reinforcement Learning of Motor Skills with Policy Gradients},
  author = {Peters, J. and Schaal, S.},
  journal = {Neural Networks},
  volume = {21},
  number = {4},
  pages = {682-697},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  month = may,
  year = {2008},
  month_numeric = {5}
}

@article{peter:article:1996,
author = {Williams, Peter},
year = {1996},
month = {06},
pages = {843-54},
title = {Using Neural Networks to Model Conditional Multivariate Densities},
volume = {8},
journal = {Neural computation},
doi = {10.1162/neco.1996.8.4.843}
}

@article{Amari:1998:NGW:287476.287477,
 author = {Amari, Shun-Ichi},
 title = {Natural Gradient Works Efficiently in Learning},
 journal = {Neural Comput.},
 issue_date = {Feb. 15, 1998},
 volume = {10},
 number = {2},
 month = feb,
 year = {1998},
 issn = {0899-7667},
 pages = {251--276},
 numpages = {26},
 url = {http://dx.doi.org/10.1162/089976698300017746},
 doi = {10.1162/089976698300017746},
 acmid = {287477},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@InProceedings{Kakade:2001,
  author =       "Kakade, Sham",
  title =        "A Natural Policy Gradient",
  booktitle =    "Advances in Neural Information Processing Systems 14 (NIPS 2001)",
  editor =    "Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin",
  year =         "2001",
  publisher = "MIT Press",
  pages =     "1531-1538",
  url = "http://books.nips.cc/papers/files/nips14/CN11.pdf",
  bib2html_rescat = "Learning Methods, MDP",
}

@inproceedings{Sutton:1999:PGM:3009657.3009806,
 author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
 series = {NIPS'99},
 year = {1999},
 location = {Denver, CO},
 pages = {1057--1063},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3009657.3009806},
 acmid = {3009806},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{Bagnell2004LearningD,
  title={Learning Decisions : Robustness , Uncertainty , and Appoximation},
  author={J. Andrew Bagnell},
  year={2004}
}

@article{DBLP:journals/corr/MetzIJD17,
  author    = {Luke Metz and
               Julian Ibarz and
               Navdeep Jaitly and
               James Davidson},
  title     = {Discrete Sequential Prediction of Continuous Actions for Deep {RL}},
  journal   = {CoRR},
  volume    = {abs/1705.05035},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.05035},
  archivePrefix = {arXiv},
  eprint    = {1705.05035},
  timestamp = {Mon, 13 Aug 2018 16:47:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MetzIJD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Stei83,
  Title                    = {The Conjugate Gradient Method and Trust Regions in Large Scale Optimization},
  Author                   = {Trond Steihaug},
  Journal                  = {SIAM Journal on Numerical Analysis},
  Year                     = {1983},
  Number                   = {3},
  Pages                    = {626--637},
  Volume                   = {20}
}

@techreport{Shewchuk:1994:ICG:865018,
 author = {Shewchuk, Jonathan R},
 title = {An Introduction to the Conjugate Gradient Method Without the Agonizing Pain},
 year = {1994},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Acmucs%3ACMU%2F%2FCS-94-125},
 publisher = {Carnegie Mellon University},
 address = {Pittsburgh, PA, USA},
} 

@InProceedings{pmlr-v37-schulman15,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@INPROCEEDINGS{Williams92simplestatistical,
    author = {Ronald J. Williams},
    title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {229--256}
}

@Article{Liu2015,
author="Liu, De-Rong
and Li, Hong-Liang
and Wang, Ding",
title="Feature selection and feature learning for high-dimensional batch reinforcement learning: A survey",
journal="International Journal of Automation and Computing",
year="2015",
month="Jun",
day="01",
volume="12",
number="3",
pages="229--242",
abstract="Tremendous amount of data are being generated and saved in many complex engineering and social systems every day. It is significant and feasible to utilize the big data to make better decisions by machine learning techniques. In this paper, we focus on batch reinforcement learning (RL) algorithms for discounted Markov decision processes (MDPs) with large discrete or continuous state spaces, aiming to learn the best possible policy given a fixed amount of training data. The batch RL algorithms with handcrafted feature representations work well for low-dimensional MDPs. However, for many real-world RL tasks which often involve high-dimensional state spaces, it is difficult and even infeasible to use feature engineering methods to design features for value function approximation. To cope with high-dimensional RL problems, the desire to obtain data-driven features has led to a lot of works in incorporating feature selection and feature learning into traditional batch RL algorithms. In this paper, we provide a comprehensive survey on automatic feature selection and unsupervised feature learning for high-dimensional batch RL. Moreover, we present recent theoretical developments on applying statistical learning to establish finite-sample error bounds for batch RL algorithms based on weighted Lpnorms. Finally, we derive some future directions in the research of RL algorithms, theories and applications.",
issn="1751-8520",
doi="10.1007/s11633-015-0893-y",
url="https://doi.org/10.1007/s11633-015-0893-y"
}

@article{HAZRATIFARD20131892,
title = "Using reinforcement learning to find an optimal set of features",
journal = "Computers \& Mathematics with Applications",
volume = "66",
number = "10",
pages = "1892 - 1904",
year = "2013",
note = "ICNC-FSKD 2012",
issn = "0898-1221",
doi = "https://doi.org/10.1016/j.camwa.2013.06.031",
url = "http://www.sciencedirect.com/science/article/pii/S0898122113004495",
author = "Seyed Mehdi Hazrati Fard and Ali Hamzeh and Sattar Hashemi",
keywords = "Feature selection, Markov decision process, Reinforcement learning, Temporal difference",
abstract = "Identifying the most characterizing features of observed data is critical for minimizing the classification error. Feature selection is the process of identifying a small subset of highly predictive features out of a large set of candidate features. In the literature, many feature selection methods approach the task as a search problem, where each state in the search space is a possible feature subset. In this study, we consider feature selection problem as a reinforcement learning problem in general and use a well-known method, temporal difference, to traverse the state space and select the best subset of features. Specifically, first, we consider the state space as a Markov decision process, and then we introduce an optimal graph search to overcome the complexity of the problem of concern. Since this approach needs a state evaluation paradigm as an aid to traverse the promising regions in the state space, the presence of a low-cost evaluation function is necessary. This method initially explores the lattice of feature sets, and then exploits the obtained experiments. Finally, two methods, based on filters and wrappers, are proposed for the ultimate selection of features. Our empirical evaluation shows that this strategy performs well in comparison with other commonly used feature selection strategies, while maintaining compatibility with all datasets in hand."
}

@Article{Sutton1988,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
issn="1573-0565",
doi="10.1007/BF00115009",
url="https://doi.org/10.1007/BF00115009"
}

@inproceedings{fellows:icml18,
  title = "Fourier Policy Gradients",
  author = "Matthew Fellows and Kamil Ciosek and Shimon Whiteson",
  year = "2018",
  booktitle = "ICML 2018: Proceedings of the Thirty-Fifth International Conference on Machine Learning",
  month = "July",
  url = "http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/fellowsicml18.pdf",
}

@techreport{1993b,
  authors       = {Leemon C. Baird III},
  title         = {Advantage Updating},
  catid1        = {MachineLearning},
  year          = {1993},
  number        = {WL--TR-93-1146},
  url           = {http://leemon.com/papers/1993b.pdf},
  institution   = {Wright-Patterson Air Force Base Ohio: Wright Laboratory}
}

@book{puterman2014markov,
  added-at = {2017-04-07T12:13:11.000+0200},
  author = {Puterman, Martin L},
  biburl = {https://www.bibsonomy.org/bibtex/22e7ac99cd30c4892171e5a7cef1bc7a7/becker},
  interhash = {6cec8f775a265d8741171d17e4a4e7d0},
  intrahash = {2e7ac99cd30c4892171e5a7cef1bc7a7},
  keywords = {inthesis diss markov chain decision process citedby:scholar:count:9594 citedby:scholar:timestamp:2017-4-7},
  publisher = {John Wiley \& Sons},
  timestamp = {2017-04-07T12:13:11.000+0200},
  title = {Markov decision processes: discrete stochastic dynamic programming},
  year = 2014
}
