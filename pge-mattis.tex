\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
\RequirePackage{fix-cm}
\documentclass[draft]{svjour3}
\smartqed
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[final]{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsmath,amssymb}
\usepackage[final]{hyperref}
\usepackage{tikz}
\usepackage{environ}
\usetikzlibrary{shapes,arrows,positioning,calc}
\begin{document}

\title{Policy Gradient Estimation}
\subtitle{A survey on policy gradient estimators}

\author {Mattis Manfred K{\"a}mmerer}

\institute{TU Darmstadt}

\date{Received: date / Accepted: date}

\maketitle

\begin{abstract}

The goal of policy gradient approaches is to find a policy in a given class of policies which maximizes the expected return.
Given a differentiable model of the policy, we want to apply a gradient-ascent technique to reach the local optimum. 
Gradient ascent has strong guarantees on convergence, and is theoretically well researched.
The main issue is that the policy gradient with respect to the expected return is not available, thus we need to estimate it.
For this reason, the biggest weakness of policy gradient algorithms is sample efficiency, as they tend to require on-policy data for the gradient estimate, and conservative learning rates for the convergence guarantees.
Thus, most research is focused on finding algorithms with improved sample efficiency.
This paper provides a deep introduction to policy gradient that shows the development of policy gradient approaches, and should enable the reader to understand current research in the area. 

\keywords{reinforcement learning \and policy gradients \and actor-critic \and natural gradient }
\end{abstract}

\section{Introduction}
\label{intro}

Policy gradient methods are approaches to maximize the expected return in a Markov Decision Process (MDP). 
Using a parameterized policy to decide the next action, they can easily incorporate prior domain knowledge, but require a lot of configuration to produce an effective agent for a specific environment. 
Also, they frequently require on-policy training and often converge slower, but can guaranteed find a local optimum given some constraints on the hyperparameters. 
Options to model a policy gradient method are not only given by the policy model itself, but also in the way a policy improvement is computed. 
In this case, improvement means a step in the parameter space such that the policy under the new parameters will on average perform better than the old policy, i.e. improve its expected return. 
Policy gradient estimation is the term we use to describe the process of computing the next step in parameter space. 
The goal is to estimate the gradient of the policy with respect to the expected return. 
Since this is the core problem of policy gradient methods, it is also the main topic of this paper.

In section \ref{sec:prel}, we give some preliminaries and describe the problem setup in detail. 
In section \ref{sec:pge}, we discuss different approaches to estimate the policy gradient. 
Using our insights from section \ref{sec:pge}, we derive the actor-critic framework in section \ref{sec:ac}, which harnesses value-function estimation for improved gradient updates. 
Then, in section \ref{sec:natural}, we introduce some gradient ascent methods that build on the approaches given in \ref{sec:pge}, refining the gradient estimation by Fisher's information matrix to get the natural gradient. 
Finally, in section \ref{sec:outro}, we give a conclusion of the reviewed approaches, and discuss potential areas of improvement.

\section{Preliminaries}
\label{sec:prel}

\paragraph{Definitions.} 
We define states $s \in \mathbb{S}$, actions $a \in \mathbb{A}$, and rewards $r \in \mathbb{R}$. 
A trajectory $\tau := (s_0, a_0, $ $s_1, a_1, \dots, s_T, a_T)$ is generated by drawing $s_0 \sim \mu_0(s_0)$ according to the distribution over initial states $\mu_0(s_0)$, and successively sampling $a_t \sim \pi_\theta(a_t|s_t)$ according to the policy $\pi_\theta$, and $s_{t+1} \sim p(s_{t+1}|s_t,a_t)$ until the horizon $T$, or a terminal state is reached. 
At each time step, we receive a reward according to $r_t = r(s_t, a_t) \equiv \mathbb{E}_{s'}\left[r(s_t,a_t,s')\right]$. 
A trajectory can also be called roll-out or episode, though the term episode implies it ends in a terminal state.
We assume a Markov Decision Process (MDP), meaning the probability distribution of the next states is independent of past states $s_{0:t-1}$ given the present state $s_t$ and action $a_t$, 
\begin{equation}
	p(s_{t+1}|s_t,a_t)=p(s_{t+1}|s_{0:t},a_{0:t}).
\end{equation}
Where we define $i:j$ with $i,j \in \mathbb{N}, i < j$ as an index over all integers from $i$ to $j$, i.e., $s_{i:j} \equiv s_i, s_{i+1}, \dots, s_j$. 
We assume no additional prior knowledge about the environment, thus we assume the probability of a trajectory is 
\begin{equation}
	p_\pi(\tau) = \mu_0(s_0) \prod_{t=0}^{T-1} p(s_{t+1}|s_t, a_t) \pi(a_t|s_t).
\end{equation}

The most frequently used policy classes in policy gradient approaches are Gibbs policies $\pi_\theta(a|s) = \frac{exp(\phi(s,a)^T\theta)}{\sum_b exp(\phi_1(s,b)^T\theta)}$ \cite{Sutton:1999:PGM:3009657.3009806,Bagnell2004LearningD} for discrete problems, and Gaussian policies $\pi_\theta(a|s) = \mathcal{N}(\phi(s,a)^T\theta_1,\theta_2)$ for continuous problems, where $\theta_2$ is an exploration parameter \cite{Williams92simplestatistical,peter:article:1996}, and $\phi(s,a)$ is the vector of basis functions on the state-action pair.

\paragraph{Policy gradient.} 
Our goal with respect to episodes is to maximize the expectation of the total reward, also called expected return. 
The total reward in the horizon $T$ is $\sum_{t=0}^{T} r_{t}$. 
We introduce a discount factor $\gamma \in [0,1)$ to trade-off bias ($\gamma\to0$) versus variance ($\gamma\to1$) with a normalization factor of $1-\gamma$, which is often omitted though strongly recommended.
Intuitively, this reflects the idea that the relevance of later actions declines, which puts more weight on less actions, thus the increased bias for reduced variance. 
The discounted total reward is 

\begin{equation}
  \mathcal{R}^\tau \equiv \mathcal{R}_0^T := (1-\gamma) \sum_{t=0}^{T} \gamma^t r_t.
  \label{eqn:acc-reward}
\end{equation}

Since we have only limited knowledge of the performance of the policy, we need to approximate an optimal policy by estimating a gradient. 
Thus, we search $\nabla_\theta J(\theta) := \nabla_\theta \mathbb{E}_{p_\pi(\tau)}\left[\mathcal{R}^\tau\right]$, to make a policy gradient step according to $\theta_{k+1} = \theta_k + \alpha_k \nabla_\theta J(\theta)$, where $\alpha_k$ denotes a learning rate. 

\section{Policy Gradient Estimation}
\label{sec:pge}

\paragraph{Finite-difference gradients.} 
A simple approach for gradient estimation is to choose a small $\delta\theta$, and evaluate the new policy given the slightly changed parameters as in 
\begin{equation}
	\nabla_\theta J(\theta) \approx \frac{J(\theta+\delta\theta)J(\theta-\delta\theta)}{2\delta\theta}.
\end{equation} 
This can lend a good estimate of the gradient given a small $\delta$, and is generally called the symmetric derivative. 
However, finite-difference gradients suffer from the curse of dimensionality, and can require very small $\delta\theta$.
Thus, they only work well in specific scenarios, but should not be discarded due to simplicity. 

\paragraph{Value functions.} 
Given we know the actual value, i.e. the expected return we will get starting from state $s_t$, this function can be used to evaluate the performance of our policy, and can be written as
\begin{equation}
	V^{\pi}(s_t) := \mathbb{E}_{\substack{s_{t+1:h} \\ a_{t:h}}}\left[\mathcal{R}_t^T\right].
	\label{eqn:v}
\end{equation}

As we will see, this function also gives us the true gradient of $J(\theta)$, though in general we need to estimate it. 
In addition to the value function, we also define the state-action value function, often called Q-function. 
Instead of the expected accumulated reward starting from state $s_t$, this function gives the expected accumulated reward given an action $a_t$ is selected in state $s_t$, 
\begin{equation}
	Q^{\pi}(s_t, a_t) := \mathbb{E}_{\substack{s_{t+1:h} \\ a_{t+1:h}}}\left[\mathcal{R}_t^T\right].
	\label{eqn:q}
\end{equation}

Using the value function, and the Q-function, we can derive a better estimate of the policy gradient.

\paragraph{Likelihood-ratio gradients.} 
For this derivation, we will change the perspective a bit, requiring some additional definitions. 
We define ${\mu_\pi}_i = \sum_{t=0}^{\infty} p(s_t=s_i | s_0, \pi)$ \cite{puterman2014markov} as the state distribution, though it does not sum up to one without normalization. Note that $\mu_\pi$ is equivalent to the discounted state visit count $d^\pi$ introduced by Sutton et al. \cite{Sutton:1999:PGM:3009657.3009806}.
Further, we define $P_\pi$ as the transition matrix, i.e. ${P_\pi}_{i,j}=p(s_j|s_i,\pi)$, $r_\pi$ as the mean rewards for all states given by ${r_\pi}_i = \int_\mathbb{A} r(s_i,a)\pi_\theta(a|s_i) da$, and $\mu_0 = \left[\mu_0(s_0),\mu_0(s_1),\ldots\right]^T$ as a vector representing the initial state distribution.
From which it follows that $V_\pi = \mu_\pi r_\pi$
From this, we derive
\begin{align}
	\mu_\pi &= \mu_0 + P_\pi^T \mu_\pi \nonumber \\
	(I-P^T)\mu_\pi &= \mu_0 \nonumber \\
	\mu_\pi^T &= \mu_0^T (I-P_\pi)^{-1} ,
\end{align}
so we can reformulate the problem as 
\begin{align}
	\underset{\theta}{\text{max}}\ J(\theta) &= \mu_0^T V_\pi \\
	\text{s.t.}\ V_\pi &= r_\pi + \gamma P_\pi V_\pi . \nonumber
\end{align}
Where $V_\pi = \left[ V_\pi(s_0), V_\pi(s_1), \ldots \right]^T$. We derive 
\begin{align*}
	\nabla_\theta J(\theta) &= \nabla_\theta \mu_0^T V_\pi
	\\ &= \mu_0^T \nabla_\theta \gamma P_\pi V_\pi
	\\ &= \mu_0^T (I - \gamma P_\pi)^{-1} (\nabla_\theta r_\pi + \gamma (\nabla_\theta P_\pi) V_\pi)
	\\ &= \mu_\pi^T (\nabla_\theta r_\pi + \gamma (\nabla_\theta P_\pi) r_\pi)
	\\ &\equiv \sum\nolimits_{i,j} d^\pi(s_i) \nabla_\theta\pi(a_j|s_i) Q(s_i,a_j)
	\\ &= \sum\nolimits_{i,j} d^\pi(s_i) \pi(a_j|s_i) \nabla_\theta\log\pi(a_j|s_i) Q_\pi(s_i,a_j).
\end{align*}
Where we used $\nabla_\theta \pi_\theta(a|s) = \pi_\theta(a|s)\nabla_\theta\log\pi_\theta(a|s)$, obtained from the likelihood ratio, which gives us the likelihood-ratio gradient,
\begin{equation}
	\nabla_\theta J(\theta) = \mathbb{E}_{\substack{\ s \sim d^\pi \\a \sim \pi}} \Big[\nabla_\theta{\log\pi_\theta(a|s)}Q_\pi(s,a)\Big].
	\label{eqn:like-grad}
\end{equation}
intuitively meaning we should to increase the probability of actions that lead to higher Q-values.

Obviously, we do not have the true $Q_\pi(s,a)$, thus we need to approximate it by $\hat{Q}_\pi(s,a)$. 
We draw $a \sim \pi(a|s)$, starting in state $s_0 \sim p(s_0)$ and match a function estimator to our observations.
It is shown that for $\lim_{k\to\infty}\alpha_k = 0$, and $\sum_{k=0}^\infty \alpha_k$ we are guaranteed to converge to a local optimum \cite{Sutton:1999:PGM:3009657.3009806}.
Approximating $Q_\pi(s,a)$ by an unbiased estimator $f_w(s_t, a_t) \equiv \hat{Q}_\pi(s_t, a_t)$, Sutton et al. \cite{Sutton:1999:PGM:3009657.3009806} show that using this function approximation we will converge to the real local optimum.

\paragraph{Immediate updates.} 
The first class of algorithms developed to update a policy directly are called REINFORCE \cite{Williams92simplestatistical}.
REINFORCE collects a complete episode, at which point we can calculate the actual state-action value by traversing backwards over the trajectory, and estimates:
\begin{equation}
	\nabla_\theta J(\theta) \approx \nabla_\theta\log\pi_\theta(s) \left(Q(s_t,a_t) - b_\tau \right).
	\label{eqn:reinforce}
\end{equation}
where $b_\tau$ is some baseline. 
Peters \& Schaal \cite{4867} find that an estimate of the optimal baseline can be calculated by
\begin{equation}
	b_\tau = \frac
		{\left\langle 
			\left(\sum_{t=0}^T \nabla_\theta \log\pi_\theta(a_t|s_t) \right)^2 \sum_{t'=0}^T a_{t'} r_{t'} 
		\right\rangle}
		{\left\langle
			\left(\sum_{t=0}^T \nabla_\theta \log\pi_\theta(a_t|s_t) \right)^2
		\right\rangle}
\end{equation}

However, this estimate is very biased.
To reduce the estimate's bias, we can compute updates from a complete episode.

\paragraph{Episodic updates.} 
A common way of reducing the bias in value estimation is to sample trajectories, also called roll-outs, and to updates using the complete roll-out. 
We call these approaches episodic. 
When we do this, we sum up the immediate updates, which we can rearrange to get
\begin{equation}
  \sum_{t=0}^T \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^T \nabla_\theta{\log\pi_\theta(a|s)}Q_\pi(s,a)\right].
  \label{eqn:egrad}
\end{equation}

Episodic REINFORCE \cite{Williams92simplestatistical}, the first approach to this concept, performs roll-outs and takes the final return, at which point we can calculate the actual state-action value by traversing backwards over the trajectory, and estimates
\begin{equation}
  \nabla_\theta J(\theta) \approx \frac{1}{T} \sum_{t=0}^T \nabla_\theta\log\pi_\theta(s) Q(s_t,a_t).
\end{equation}
This is sometimes also called Monte-Carlo gradient estimation. 
Given $r_t > 0, \forall t=0,\dots,h$, these approaches can only increase action probabilities. 
Obviously, we normalize to ensure $\forall s \in \mathbb{S}: \int_\mathbb{A}{\pi(a|s)da} = 1$. 
This means actions can only become less probable in relation to other actions. 
We find that this introduces more variance when learning from samples \cite{Sutton:1999:PGM:3009657.3009806}. 
One approach to counter the variance is to introduce a baseline
\begin{equation}
  \hat{Q}(s,a) \approx \gamma^{t} (r_{t} - b(s_t)).
\end{equation}
The baseline can represent any prior knowledge or assumption about states. 
A common way to use this baseline is to use the value function, which gives us the so-called advantage function

\begin{equation}
	A_{\pi}(s_t, a_t) := Q_{\pi}(s_t, a_t) - V_{\pi}(s_t).
	\label{eqn:adv}
\end{equation}

Whenever we require estimating a value function for updating our policy, we can name the policy actor, and the estimated value function critic. 
From this observation, we define a class of policy optimization methods called actor-critic methods in section \ref{sec:ac}.


\section{Actor-Critic Methods} 
\label{sec:ac}

\tikzset{block/.style= {draw, rectangle, align=center, minimum height=2em, minimum width=3cm}}
\begin{figure}
  \begin{tikzpicture}[auto, node distance=2cm,>=latex']
    \node [block, name=env, minimum width=6cm] (env) {Environment};
    \node [block, above of=env, node distance=1.5cm] (critic) {Critic \\ $\hat{Q}^\pi(s,a)$};
    \node [block, above of=critic] (actor) {Actor \\ policy $\pi(a|s)$};
    
    \draw[densely dotted] ($(actor.north west)+(-1.5,0.6)$) rectangle ($(critic.south east)+(1.5,-0.15)$);
    \node[above of=actor, node distance=0.7cm] {Agent};

    \draw [->, align=left, swap, densely dashed] (critic.north) -- node{Policy \\ Improvement} (actor.south);
    \draw [->, pos=0.75] ($(env.north west)!0.15!(env.north)$) |- node{$s_t$} (actor.west);
    \draw [->, pos=0.75] ($(env.north west)!0.15!(env.north)$) |- node{$s_t, r_t$} (critic.west);
    \draw [->, pos=0.25] (actor.east) -| node{$a_t$} ($(env.north east)!0.15!(env.north)$);
    \draw [->, pos=0.75, swap] ($(env.north east)!0.15!(env.north)$) |- node{$a_t$} (critic.east);
    
    \draw [-, swap, pos=0.14] ($(env.north west)!0.15!(env.north)$) |- node{Observation} (critic.west);
    \draw [-, swap, pos=0.95] (actor.east) -| node{Action} ($(env.north east)!0.15!(env.north)$);
  \end{tikzpicture}
  \caption{An actor-critic framework comparable to Kimura et al.  \cite{Kimura1998AnAO}.} \label{fig:ac}
\end{figure}

Policy gradient methods can be described in terms of two main steps often called policy evaluation and policy improvement. 
For actor-critic approaches, we separate these steps from the actor component by implementing a critic. 
This means, the actor consists only of the policy, while the critic is focused on estimating a score for the actions taken. 
By that concept, observations of the environment are given to the actor only to decide the next action, and to the critic only to improve its function estimation with the respective rewards. 
Figure \ref{fig:ac} shows the general structure of an actor-critic algorithm.

The critic estimates a state-action value function as defined in \eqref{eqn:q}. 
\cite{Sutton:1999:PGM:3009657.3009806}, and \cite{Konda:2003:AA:942271.942292} find that the estimation $f_w^\pi(s,a) \approx Q_\pi(s,a)$ does not affect the unbiasedness of the gradient estimate under some restrictions. 
Specifically, this holds for $f_w^\pi(s,a) = {\nabla_\theta \log\pi(a|s)}^T w$, thus $f_w^\pi(s,a)$ being a linear function parameterized by the vector $w$.
This guarantees that the function estimator does not cause divergence, and really enables recent research in reinforcement learning for continuous control problems, e.g., in humanoid robotics.

Traditionally, the improvement is often done by Monte-Carlo sampling as in REINFORCE \eqref{eqn:reinforce}, or using temporal difference (TD) \cite{Sutton1988}, i.e., we use the temporal difference between the critic's estimations 
\begin{equation}
  r_t + \gamma \hat{V}_\pi(s_{t+1}) - \hat{V}_\pi(s_t).
\end{equation}

However, Sutton et al. \cite{1993b} find that this is only guaranteed to be unbiased, if $\int_\mathbb{S}{\pi(s,a)f_w(s,a)da} = 0$ for each state. 
Given this assumption, the function estimator $f_w$ is limited to an advantage function \eqref{eqn:adv}, which requires bootstrapping for $V_\pi(s,a)$. 
If we use temporal difference in this context, we run into a problem, as \eqref{eqn:adv} subtracts $\hat{V}_\pi(s_t)$, meaning we would only learn immediate rewards \cite{Peters_IICHR_2003}. 
This would render the process biased. 
Sutton et al. \cite{Sutton:1999:PGM:3009657.3009806} and Konda et al. \cite{NIPS1999_1786} suggest estimating an action value function as in \eqref{eqn:q}. 
We can approximate this $f_w^\pi$ by least-squares optimization over multiple $\hat{Q}_\pi(s,a)$ obtained from roll-outs. 
However, Peters et al. \cite{4863} find that this approximation is highly reliant on the distribution of the training data. 
This comes from the realization, that we use only a subspace of the actual action value function in $f_w^\pi$, which is only a state value function. 
One can compare this to approximating a parabola by a line, whereby the approximation changes wildly depending on which part of the parabola is in the training data. 
An approach to solve this bootstrapping problem is to rewrite the Bellman Equation using \eqref{eqn:adv} and \eqref{eqn:q} with $A_\pi(s,a) = f_w^\pi(s,a)$, $V_\pi(s) = \phi(s)^T v$, and a zero-mean error term $\epsilon(s_t,a_t,s_{t+1})$, we get
\begin{align}
  Q_\pi(s,a) = A_\pi(s,a) + V_\pi(s) &= r(s,a) + \gamma \int_\mathbb{S} p(s'|s,a)V_\pi(s')ds' \\
  \nabla_\theta \log \pi(a_t|s_t)^T w + \phi(s_t)^T v &= r(s_t,a_t) + \gamma \phi(s_{t+1})^T v + \epsilon(s_t,a_t,s_{t+1})
\end{align}
which gives involves only linear equations to solve. \cite{4863} 

With these insights in mind, section \ref{sec:natural} presents the natural gradient, a refined type of gradient which has a convenient fit in the actor-critic setting we just established.

\section{Natural Gradient}
\label{sec:natural}

Natural gradients were at first proposed for use in supervised learning settings by Amari et al. \cite{Amari:1998:NGW:287476.287477}, but have been shown to be effective in reinforcement learning by Kakade \cite{Kakade:2001} and Peters et al. \cite{4863}.

When using normal gradient steps, we find that steps can become very small when a plateau is reached. 
This can will drastically slow down the learning process, and in the worst case cause some algorithms to prematurely. 
However, we can use some additional information to refine the gradient. 
Figure \ref{fig:nat-grad-adv} shows an example by Peters et al. \cite{Peters_IICHR_2003} that gives a visual intuition about the difference between 'vanilla' and natural policy gradients. 

\begin{figure}
  \includegraphics[width=\textwidth]{nat-grad-adv}
  \caption{An experiment showing where the natural gradient has a great advantage. \cite{Peters_IICHR_2003} }\label{fig:nat-grad-adv}
\end{figure}

Using the Fisher information matrix $F_\theta$, and the gradient estimate we discussed in section \ref{sec:pge} gives us the definition
\begin{equation}
  \widetilde{\nabla}_\theta J(\theta) := F^{-1}_\theta \nabla_\theta J(\theta)
  \label{eqn:nat-grad}
\end{equation}
of the natural gradient. 
The Fisher information matrix represents the certainty we have on our estimate of the gradient and is defined as the covariance of the log likelihood function of a trajectory $\tau_{\pi_\theta}^T$, which as \cite{4863} show can be written as 
\begin{equation}
  F_\theta = \int_\mathbb{S} d^\pi(s) \int_\mathbb{A} \pi_\theta(a|s) \nabla_\theta \log{\pi_\theta(a|s)} \nabla_\theta \log{\pi_\theta(a|s)}^T dads.
  \label{eqn:F}
\end{equation}
Using a value function estimator and calculating the natural gradient, we get the natural policy gradient algorithm (NPG) \cite{NIPS2017_7233}.
But, if we recall the definition \eqref{eqn:like-grad} of likelihood-ratio gradients, we see that setting $\hat{Q}(s,a) := \nabla_\theta \log \pi(a|s)^T w$, we get
\begin{equation}
  \nabla_\theta J(\theta) = \int_\mathbb{S} {d^\pi(s) \int_\mathbb{A} \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) {\nabla_\theta \log \pi_\theta(a|s)}^T dads}\ w \equiv F_\theta w.
  \label{eqn:J-equals-F}
\end{equation}
From \eqref{eqn:nat-grad}, \eqref{eqn:F}, and \eqref{eqn:J-equals-F}, it follows that
\begin{equation}
  \widetilde{\nabla}_\theta J(\theta) = F^{-1}_\theta \nabla_\theta J(\theta) = F_\theta^{-1} F_\theta w = w.
\end{equation}
Thus, this approach does not require an actual estimate of the Fisher information matrix, but only an estimate of $w$, with the update step according to $\theta_{k+1} = \theta_k + \alpha_k w$.

Peters et al. \cite{4863} present this idea and suggest LSTD($\lambda$)-Q, a version of least-squares temporal difference learning  \cite{Boyan:1999:LTD:645528.657618}, as well as episodic natural actor-critic (eNAC).

\section{Conclusion}
\label{sec:outro}

In this paper, we have introduced policy gradient methods as a class of reinforcement learning algorithms. 
We show why policy gradient methods are effective in these environments, and we give some intuitions about the concept. 
Further, we show the core elements of policy gradient methods, discuss some intricacies the estimation of the policy gradient brings, and follow the research development in the attempts of improving the efficiency and stability of policy gradients.
We show that we can reuse value-estimation approaches in actor-critic settings to improve gradient estimate through better policy evaluation.
This leads to the introduction of the natural gradient as a way to iterate through policy space instead of parameter space, which improves sample efficiency, especially when the gradient in parameter space is very small, i.e., if there is a plateau.

From the developments in recent research, it is fair to say that policy gradient methods play a major role in reinforcement learning. 

\bibliographystyle{spmpsci}
\bibliography{bibliography}

\end{document}
